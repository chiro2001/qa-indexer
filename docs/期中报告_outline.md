# outline

[toc]

## intro

1. Seq2Seq Model becoming more and more popular
2. From CNN/RNN to Transformers
3. BERT: Pretrained model can be used to fine-tuning specific model to apply NLP tech to smaller application scenario

## details

1. Transformer principle: Attention is all you need
2. Transformer usage in QA model
3. `chinese-roberta-wwm-ext`
4. use transformer example code

## innovation

1. create 

## our work

1. collect train & test data
2. study transformer/BERT principle and usage

## summary

